# AI Monitoring and Progress Tracking

The AI controller has extensive built-in monitoring capabilities to track learning progress, save states, and visualize performance. Here's everything you can monitor:

## ðŸŽ¯ What the AI Tracks

### Real-Time Statistics
- **Episode Count**: Total training episodes completed
- **Step Count**: Total moves made across all episodes
- **Current Epsilon**: Exploration vs exploitation balance (1.0 = random, 0.0 = optimal)
- **Best Score**: Highest reward achieved in any episode
- **Memory Usage**: Experience replay buffer utilization

### Per-Episode Metrics
- **Reward**: Total reward earned (entropy reduction + solve bonus + penalties)
- **Steps**: Moves taken to complete/timeout
- **Solved**: Whether puzzle was solved completely
- **Final Entropy**: Manhattan distance sum at episode end
- **Epsilon**: Exploration rate at time of episode

### Learning Progress Indicators
- **Solve Rate**: Percentage of puzzles solved (rolling windows)
- **Average Reward**: Mean reward over time (trending upward = learning)
- **Solution Efficiency**: Steps to solve (trending downward = getting better)
- **Learning Velocity**: Rate of improvement in rewards
- **Convergence**: Stability of Q-values (less exploration needed)

## ðŸ“Š Visualization Options

### 1. Real-Time Console Monitoring
```bash
# Monitor during training with verbose output
python train_ai.py --episodes 1000 --verbose

# Example output:
Episode 150/1000 | Reward: 847.3 | Steps: 89 | Solved: Yes |
Solve Rate: 23.3% | Avg Reward (10): 234.1 | Epsilon: 0.345 | Time: 145.2s
```

### 2. Comprehensive Training Plots
```bash
# Generate full visualization after training
python train_ai.py --episodes 500 --plot

# Or analyze saved model
python monitor_ai.py --stats model_stats.json --plot
```

**Six-Panel Dashboard:**
1. **Reward Progression**: Episode rewards with moving average
2. **Solve Rate Over Time**: Rolling window success rate
3. **Solution Efficiency**: Steps to solve (scatter + trend)
4. **Exploration Decay**: Epsilon reduction over time
5. **Final State Quality**: Entropy distribution histogram
6. **Learning Velocity**: Rate of reward improvement

### 3. Interactive Dashboard
```bash
python monitor_ai.py --stats final_model_4x4_1726384567_stats.json --plot
```

### 4. Live Training Monitor
```bash
# Real-time updates during training (10-second intervals)
python monitor_ai.py --live --interval 10
```

## ðŸ’¾ Saved Learning States

### Automatic Saves During Training
The AI automatically saves:

1. **Model Weights** (`model_name.h5` or `model_name_qtable.json`)
   - Neural network weights (TensorFlow) or Q-table values
   - Can be loaded to resume training or for evaluation

2. **Training Statistics** (`model_name_stats.json`)
   - Complete episode-by-episode history
   - Model hyperparameters
   - Performance metrics
   - Training timestamps

3. **Periodic Checkpoints** (every 100 episodes by default)
   - Intermediate saves during long training runs
   - Recover from interruptions
   - Compare performance at different stages

### Example Saved Statistics Structure
```json
{
  "episode_count": 1000,
  "step_count": 45234,
  "best_score": 1847.3,
  "epsilon": 0.01,
  "training_history": [
    {
      "episode": 1,
      "reward": -45.2,
      "steps": 200,
      "solved": false,
      "epsilon": 0.99,
      "final_entropy": 142
    },
    ...
  ],
  "model_parameters": {
    "learning_rate": 0.001,
    "discount_factor": 0.95,
    "epsilon_decay": 0.995,
    "breadth": 4
  }
}
```

## ðŸ“ˆ Performance Analysis

### Key Performance Indicators

**Early Training (Episodes 1-100):**
- Solve Rate: 0-5%
- Average Reward: -50 to 0
- Epsilon: 1.0 â†’ 0.6
- Steps to Solve: 150+ (when solved)

**Mid Training (Episodes 100-500):**
- Solve Rate: 5-30%
- Average Reward: 0 to 400
- Epsilon: 0.6 â†’ 0.2
- Steps to Solve: 100-150

**Advanced Training (Episodes 500-1000+):**
- Solve Rate: 30-80%
- Average Reward: 400-800
- Epsilon: 0.2 â†’ 0.01
- Steps to Solve: 50-100

**Expert Level (1000+ Episodes):**
- Solve Rate: 70-90%
- Average Reward: 600-1000
- Epsilon: 0.01 (minimal exploration)
- Steps to Solve: 30-80

### What to Look For

**ðŸŸ¢ Good Learning Signs:**
- Solve rate steadily increasing
- Average reward trending upward
- Steps to solve decreasing
- Epsilon decaying smoothly
- Final entropy distribution shifting left (toward 0)

**ðŸŸ¡ Concerning Signs:**
- Solve rate plateau for 200+ episodes
- Reward oscillating wildly
- Steps to solve not improving
- Too fast epsilon decay (premature convergence)

**ðŸ”´ Problem Indicators:**
- Solve rate decreasing
- Rewards trending downward
- AI getting stuck in loops
- Epsilon stuck at high values
- Final entropy distribution not improving

## ðŸ”§ Monitoring Commands

### Basic Progress Check
```bash
# Quick summary of saved model
python monitor_ai.py --stats final_model_4x4_1726384567_stats.json

# Output:
AI TRAINING SUMMARY
===================
Total Episodes: 1000
Total Steps: 45234
Best Score: 1847.3
Current Epsilon: 0.0100

Overall Solve Rate: 67.3%
Recent Solve Rate (last 100): 84.2%
Average Reward: 423.1
Learning Progress: +61.9% improvement
```

### Detailed Analysis
```bash
# Full model analysis with plots
python monitor_ai.py --stats model_stats.json --plot --save-plot analysis.png

# Model architecture inspection
python monitor_ai.py --model final_model_4x4_1726384567
```

### Compare Models
```bash
# Train multiple models with different parameters
python train_ai.py --episodes 500 --learning-rate 0.001
python train_ai.py --episodes 500 --learning-rate 0.0005

# Compare their statistics files
python monitor_ai.py --stats model1_stats.json --plot
python monitor_ai.py --stats model2_stats.json --plot
```

### Live Training with Monitoring
```bash
# Terminal 1: Start training
python train_ai.py --episodes 2000 --verbose --save-freq 50

# Terminal 2: Monitor live (while training)
python monitor_ai.py --live --interval 15
```

## ðŸŽ® Interactive Evaluation

### Watch AI Play
```bash
# Interactive demo with trained model
python train_ai.py --load final_model_4x4_1726384567 --interactive

# Shows:
# - Current puzzle state
# - Available moves
# - AI's choice with reasoning
# - Entropy changes
# - Step-by-step solution process
```

### Evaluation Metrics
```bash
# Comprehensive evaluation on 100 fresh puzzles
python train_ai.py --load final_model_4x4_1726384567 --evaluate-only

# Output:
Evaluation Results (100 episodes):
Solve Rate: 87.0%
Average Steps: 64.3
Average Reward: 756.2
Average Solve Time: 58.7 steps
Average Final Entropy: 8.4
```

## ðŸ“‹ Monitoring Checklist

**During Training:**
- [ ] Monitor solve rate progression (should increase)
- [ ] Watch reward trends (should trend upward)
- [ ] Check epsilon decay (should decrease smoothly)
- [ ] Observe solution efficiency (steps should decrease)
- [ ] Verify memory usage (experience replay filling up)

**After Training:**
- [ ] Generate comprehensive plots
- [ ] Evaluate on fresh test set
- [ ] Compare with baseline/previous models
- [ ] Save model and statistics
- [ ] Document hyperparameters and results

**Before Deployment:**
- [ ] Test interactive mode
- [ ] Verify model loads correctly
- [ ] Check performance consistency
- [ ] Document model capabilities and limitations

The AI provides complete transparency into its learning process - you can see exactly what it's learning, how fast it's improving, and whether it's truly understanding the puzzle's deceptive complexity!